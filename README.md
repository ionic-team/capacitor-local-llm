# @capacitor/local-llm

Capacitor Local LLM plugin

## Install

```bash
npm install @capacitor/local-llm
npx cap sync
```

## API

<docgen-index>

* [`systemAvailability()`](#systemavailability)
* [`download()`](#download)
* [`prompt(...)`](#prompt)
* [`endSession(...)`](#endsession)
* [`generateImage(...)`](#generateimage)
* [Interfaces](#interfaces)
* [Type Aliases](#type-aliases)

</docgen-index>

<docgen-api>
<!--Update the source file JSDoc comments and rerun docgen to update the docs below-->

The main plugin interface for interacting with on-device LLMs.

### systemAvailability()

```typescript
systemAvailability() => Promise<SystemAvailabilityResponse>
```

Checks the availability status of the on-device LLM.

Use this method to determine if the LLM is ready to use, needs to be downloaded,
or is unavailable on the device.

**Returns:** <code>Promise&lt;<a href="#systemavailabilityresponse">SystemAvailabilityResponse</a>&gt;</code>

**Since:** 1.0.0

--------------------


### download()

```typescript
download() => Promise<void>
```

Downloads the on-device LLM model.

This method initiates the download of the LLM model when it's not already
present on the device. Only available on Android.

**Since:** 1.0.0

--------------------


### prompt(...)

```typescript
prompt(options: PromptOptions) => Promise<PromptResponse>
```

Sends a prompt to the on-device LLM and receives a response.

Use this method to interact with the LLM. You can optionally provide a sessionId
to maintain conversation context across multiple prompts.

| Param         | Type                                                    | Description                                                               |
| ------------- | ------------------------------------------------------- | ------------------------------------------------------------------------- |
| **`options`** | <code><a href="#promptoptions">PromptOptions</a></code> | - The prompt options including the text prompt and optional configuration |

**Returns:** <code>Promise&lt;<a href="#promptresponse">PromptResponse</a>&gt;</code>

**Since:** 1.0.0

--------------------


### endSession(...)

```typescript
endSession(options: EndSessionOptions) => Promise<void>
```

Ends an active LLM session.

Use this method to clean up resources when you're done with a conversation session.
This is important for managing memory and preventing resource leaks.

| Param         | Type                                                            | Description                                   |
| ------------- | --------------------------------------------------------------- | --------------------------------------------- |
| **`options`** | <code><a href="#endsessionoptions">EndSessionOptions</a></code> | - The options containing the sessionId to end |

**Since:** 1.0.0

--------------------


### generateImage(...)

```typescript
generateImage(options: GenerateImageOptions) => Promise<GenerateImageResponse>
```

Generates images from a text prompt using the on-device LLM.

Use this method to create images based on text descriptions. The generated
images are returned as base64-encoded PNG strings in an array.

| Param         | Type                                                                  | Description                                                            |
| ------------- | --------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **`options`** | <code><a href="#generateimageoptions">GenerateImageOptions</a></code> | - The image generation options including the prompt and optional count |

**Returns:** <code>Promise&lt;<a href="#generateimageresponse">GenerateImageResponse</a>&gt;</code>

**Since:** 1.0.0

--------------------


### Interfaces


#### SystemAvailabilityResponse

Response containing the system availability status of the on-device LLM.

| Prop         | Type                                                        | Description                                 | Since |
| ------------ | ----------------------------------------------------------- | ------------------------------------------- | ----- |
| **`status`** | <code><a href="#llmavailability">LLMAvailability</a></code> | The current availability status of the LLM. | 1.0.0 |


#### PromptResponse

Response from the LLM after processing a prompt.

| Prop       | Type                | Description                             | Since |
| ---------- | ------------------- | --------------------------------------- | ----- |
| **`text`** | <code>string</code> | The text response generated by the LLM. | 1.0.0 |


#### PromptOptions

Options for sending a prompt to the LLM.

| Prop               | Type                                              | Description                                                                                                                                                                                       | Since |
| ------------------ | ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- |
| **`sessionId`**    | <code>string</code>                               | Optional session identifier for maintaining conversation context. Provide the same sessionId across multiple prompts to maintain context. If not provided, each prompt is treated as independent. | 1.0.0 |
| **`instructions`** | <code>string</code>                               | System-level instructions to guide the LLM's behavior. Use this to set the role, tone, or constraints for the LLM's responses.                                                                    | 1.0.0 |
| **`options`**      | <code><a href="#llmoptions">LLMOptions</a></code> | Configuration options for controlling LLM inference behavior.                                                                                                                                     | 1.0.0 |
| **`prompt`**       | <code>string</code>                               | The text prompt to send to the LLM.                                                                                                                                                               | 1.0.0 |


#### LLMOptions

Configuration options for LLM inference behavior.

| Prop                       | Type                | Description                                                                                                                                                           | Since |
| -------------------------- | ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- |
| **`temperature`**          | <code>number</code> | Controls randomness in the model's output. Higher values (e.g., 0.8) make output more random, while lower values (e.g., 0.2) make it more focused and deterministic.  | 1.0.0 |
| **`maximiumOutputTokens`** | <code>number</code> | The maximum number of tokens to generate in the response. Note: This property name contains a typo ("maximium" instead of "maximum") but is kept for API consistency. | 1.0.0 |


#### EndSessionOptions

Options for ending an active LLM session.

| Prop            | Type                | Description                                                                                            | Since |
| --------------- | ------------------- | ------------------------------------------------------------------------------------------------------ | ----- |
| **`sessionId`** | <code>string</code> | The identifier of the session to end. This should match the sessionId used in previous prompt() calls. | 1.0.0 |


#### GenerateImageResponse

Response containing the generated image data.

| Prop                  | Type                  | Description                                                                                                                                                                          | Since |
| --------------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----- |
| **`pngBase64Images`** | <code>string[]</code> | Array of generated images as base64-encoded PNG strings. Each string contains raw base64 data (without data URI prefix). To use in an img tag, prefix with 'data:image/png;base64,'. | 1.0.0 |


#### GenerateImageOptions

Options for generating an image from a text prompt.

| Prop               | Type                  | Description                                                                 | Default        | Since |
| ------------------ | --------------------- | --------------------------------------------------------------------------- | -------------- | ----- |
| **`prompt`**       | <code>string</code>   | The text prompt describing the image to generate.                           |                | 1.0.0 |
| **`promptImages`** | <code>string[]</code> |                                                                             |                |       |
| **`count`**        | <code>number</code>   | The number of image variations to generate. Defaults to 1 if not specified. | <code>1</code> | 1.0.0 |


### Type Aliases


#### LLMAvailability

Availability status of the on-device LLM.

<code>'available' | 'unavailable' | 'notready' | 'downloadable' | 'responding'</code>

</docgen-api>
