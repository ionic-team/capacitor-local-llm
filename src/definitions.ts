/**
 * The main plugin interface for interacting with on-device LLMs.
 *
 * @since 1.0.0
 * @platform ios
 * @platform android
 */
export interface LocalLLMPlugin {
  /**
   * Checks the availability status of the on-device LLM.
   *
   * Use this method to determine if the LLM is ready to use, needs to be downloaded,
   * or is unavailable on the device.
   *
   * @since 1.0.0
   * @example
   * ```typescript
   * const { status } = await LocalLLM.systemAvailability();
   * ```
   * @returns A promise that resolves with the system availability status
   */
  systemAvailability(): Promise<SystemAvailabilityResponse>;

  /**
   * Downloads the on-device LLM model.
   *
   * This method initiates the download of the LLM model when it's not already
   * present on the device. Only available on Android.
   *
   * @since 1.0.0
   * @platform android
   * @example
   * ```typescript
   * await LocalLLM.download();
   * ```
   * @returns A promise that resolves when the download completes
   */
  download(): Promise<void>;

  /**
   * Sends a prompt to the on-device LLM and receives a response.
   *
   * Use this method to interact with the LLM. You can optionally provide a sessionId
   * to maintain conversation context across multiple prompts.
   *
   * @since 1.0.0
   * @example
   * ```typescript
   * const response = await LocalLLM.prompt({ prompt: 'What is the capital of France?' });
   * ```
   * @param options - The prompt options including the text prompt and optional configuration
   * @returns A promise that resolves with the LLM's text response
   */
  prompt(options: PromptOptions): Promise<PromptResponse>;

  /**
   * Ends an active LLM session.
   *
   * Use this method to clean up resources when you're done with a conversation session.
   * This is important for managing memory and preventing resource leaks.
   *
   * @since 1.0.0
   * @example
   * ```typescript
   * await LocalLLM.endSession({ sessionId: 'session-123' });
   * ```
   * @param options - The options containing the sessionId to end
   * @returns A promise that resolves when the session is ended
   */
  endSession(options: EndSessionOptions): Promise<void>;
}

/**
 * Configuration options for LLM inference behavior.
 *
 * @since 1.0.0
 */
export interface LLMOptions {
  /**
   * Controls randomness in the model's output.
   *
   * Higher values (e.g., 0.8) make output more random, while lower values
   * (e.g., 0.2) make it more focused and deterministic.
   *
   * @since 1.0.0
   */
  temperature?: number;

  /**
   * The maximum number of tokens to generate in the response.
   *
   * Note: This property name contains a typo ("maximium" instead of "maximum")
   * but is kept for API consistency.
   *
   * @since 1.0.0
   */
  maximiumOutputTokens?: number;
}

/**
 * Options for sending a prompt to the LLM.
 *
 * @since 1.0.0
 */
export interface PromptOptions {
  /**
   * Optional session identifier for maintaining conversation context.
   *
   * Provide the same sessionId across multiple prompts to maintain context.
   * If not provided, each prompt is treated as independent.
   *
   * @since 1.0.0
   */
  sessionId?: string;

  /**
   * System-level instructions to guide the LLM's behavior.
   *
   * Use this to set the role, tone, or constraints for the LLM's responses.
   *
   * @since 1.0.0
   * @example 'You are a helpful assistant that provides concise answers.'
   */
  instructions?: string;

  /**
   * Configuration options for controlling LLM inference behavior.
   *
   * @since 1.0.0
   */
  options?: LLMOptions;

  /**
   * The text prompt to send to the LLM.
   *
   * @since 1.0.0
   */
  prompt: string;
}

/**
 * Response from the LLM after processing a prompt.
 *
 * @since 1.0.0
 */
export interface PromptResponse {
  /**
   * The text response generated by the LLM.
   *
   * @since 1.0.0
   */
  text: string;
}

/**
 * Response containing the system availability status of the on-device LLM.
 *
 * @since 1.0.0
 */
export interface SystemAvailabilityResponse {
  /**
   * The current availability status of the LLM.
   *
   * @since 1.0.0
   */
  status: LLMAvailability;
}

/**
 * Options for ending an active LLM session.
 *
 * @since 1.0.0
 */
export interface EndSessionOptions {
  /**
   * The identifier of the session to end.
   *
   * This should match the sessionId used in previous prompt() calls.
   *
   * @since 1.0.0
   */
  sessionId: string;
}

/**
 * Availability status of the on-device LLM.
 *
 * @since 1.0.0
 */
export type LLMAvailability = 'available' | 'unavailable' | 'notready' | 'downloadable' | 'responding';
